{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import gym\n",
    "import hiive.mdptoolbox\n",
    "import hiive.mdptoolbox.mdp\n",
    "import hiive.mdptoolbox.example\n",
    "\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "import hiive_openAI_extract\n",
    "\n",
    "# import hiive.mdptoolbox as mdptoolbox\n",
    "from hiive.mdptoolbox.mdp import ValueIteration, PolicyIterationModified, QLearning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# set seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    b'S': 'b',\n",
    "    b'F': 'w',\n",
    "    b'H': 'k',\n",
    "    b'G': 'g'\n",
    "}\n",
    "\n",
    "directions = {\n",
    "            0: '←',\n",
    "            1: '↓',\n",
    "            2: '→',\n",
    "            3: '↑'\n",
    "}\n",
    "\n",
    "def plot_lake(env, policy=None, title=\"Frozen Lake\", flag=True):\n",
    "    squares = env.nrow\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111, xlim=(-.01, squares+0.01), ylim=(-.01, squares+0.01))\n",
    "    plt.title(title, fontsize=16, weight='bold', y=1.01)\n",
    "    for i in range(squares):\n",
    "        for j in range(squares):\n",
    "            y = squares - i - 1\n",
    "            x = j\n",
    "            p = plt.Rectangle([x, y], 1, 1, linewidth=1, edgecolor='k')\n",
    "            p.set_facecolor(colors[env.desc[i,j]])\n",
    "            ax.add_patch(p)\n",
    "            \n",
    "            if policy is not None:\n",
    "                text = ax.text(x+0.5, y+0.5, directions[policy[i, j]],\n",
    "                               horizontalalignment='center', size=25, verticalalignment='center',\n",
    "                               color='k')\n",
    "            \n",
    "    plt.savefig(title + '.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code based on:\n",
    "# https://medium.com/analytics-vidhya/solving-the-frozenlake-environment-from-openai-gym-using-value-iteration-5a078dffe438\n",
    "def get_score(env, policy, printInfo=False, episodes=1000):\n",
    "    misses = 0\n",
    "    successes = 0\n",
    "    steps_list = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        steps=0\n",
    "        while True:\n",
    "            action = policy[observation]\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            steps+=1\n",
    "            if done and reward == 1:\n",
    "                # print('You have got the Frisbee after {} steps'.format(steps))\n",
    "                steps_list.append(steps)\n",
    "                successes += 1\n",
    "                break\n",
    "            elif done and reward == 0:\n",
    "                # print(\"You fell in a hole!\")\n",
    "                misses += 1\n",
    "                break\n",
    "    ave_steps = np.mean(steps_list)\n",
    "    std_steps = np.std(steps_list)\n",
    "#     pct_fail  = (misses/episodes)* 100\n",
    "    pct_success  = (successes/episodes)* 100\n",
    "    \n",
    "    if (printInfo):\n",
    "        print('----------------------------------------------')\n",
    "        print('You took an average of {:.0f} steps to get the frisbee'.format(ave_steps))\n",
    "        print('And you fell in the hole {:.2f} % of the times'.format(pct_fail))\n",
    "        print('----------------------------------------------')\n",
    "  \n",
    "    return ave_steps, std_steps, pct_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the problem - just one time\n",
    "random_map = generate_random_map(size=5, p=0.8)\n",
    "env = gym.make('FrozenLake-v1', desc=random_map).unwrapped\n",
    "\n",
    "env.max_episode_steps=250\n",
    "\n",
    "# Create transition and reward matrices from OpenAI P matrix\n",
    "rows = env.nrow\n",
    "cols = env.ncol\n",
    "T = np.zeros((4, rows*cols, rows*cols))\n",
    "R = np.zeros((4, rows*cols, rows*cols))\n",
    "\n",
    "old_state = np.inf\n",
    "\n",
    "for square in env.P:\n",
    "    for action in env.P[square]:\n",
    "        for i in range(len(env.P[square][action])):\n",
    "            new_state = env.P[square][action][i][1]\n",
    "            if new_state == old_state:\n",
    "                T[action][square][env.P[square][action][i][1]] = T[action][square][old_state] + env.P[square][action][i][0]\n",
    "                R[action][square][env.P[square][action][i][1]] = R[action][square][old_state] + env.P[square][action][i][2]\n",
    "            else:\n",
    "                T[action][square][env.P[square][action][i][1]] = env.P[square][action][i][0]\n",
    "                R[action][square][env.P[square][action][i][1]] = env.P[square][action][i][2]\n",
    "            old_state = env.P[square][action][i][1]\n",
    "\n",
    "print(env.nrow, env.ncol)\n",
    "plot_lake(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_list = []\n",
    "# stepstd_list = []\n",
    "# success_list = []\n",
    "# for gamma in range(50, 100, 5):\n",
    "#     vi_small = ValueIteration(T, R, (gamma + 0.5) / 100, epsilon=0.001)\n",
    "#     vi_small.run()\n",
    "#     print(vi_small.policy)\n",
    "#     steps, steps_stddev, successs = get_score(env, vi_small.policy)\n",
    "#     step_list.append(steps)\n",
    "#     stepstd_list.append(steps_stddev)\n",
    "#     success_list.append(successs)\n",
    "\n",
    "# # sns.set(style=\"whitegrid\")\n",
    "# gamma_arr = [i / 100 for i in range(50, 100, 5)]   \n",
    "\n",
    "# fig = plt.figure(figsize=(10,4))\n",
    "# ax  = sns.barplot(gamma_arr,step_list) \n",
    "# ax.set_title('Average Steps when selecting different gamma')\n",
    "# ax.set_xlabel('Gamma')\n",
    "# ax.set_ylabel('Average Steps')\n",
    "\n",
    "# title='VI_averageSteps_vs_gamma'\n",
    "# plt.savefig(title + '.png', dpi=400)\n",
    "\n",
    "# fig = plt.figure(figsize=(10,4))\n",
    "# ax  = sns.barplot(gamma_arr,success_list) \n",
    "# ax.set_title('Success rate when selecting different gamma (percentage %)')\n",
    "# ax.set_xlabel('Gamma')\n",
    "# ax.set_ylabel('Success rate/%')\n",
    "\n",
    "# title='VI_successRate_vs_gamma'\n",
    "# plt.savefig(title + '.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for value iteration\n",
    "vi_small = ValueIteration(T, R, 0.95, epsilon=0.001)\n",
    "vi_small.run()\n",
    "best_policy = np.array(list(vi_small.policy))\n",
    "\n",
    "\n",
    "# bestID = argmax(success_list)\n",
    "# best_policy = vi_data['policy'][bestRun]\n",
    "# best_policy = np.array(list(policy_list[bestID]))\n",
    "\n",
    "rows = env.nrow\n",
    "cols = env.ncol\n",
    "\n",
    "best_policy = best_policy.reshape(rows, cols)\n",
    "\n",
    "# plot the policy\n",
    "title='Frozen Lake VI Optimal Policy'\n",
    "plot_lake(env, best_policy, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for policy iteration\n",
    "\n",
    "pi_small = PolicyIterationModified(T, R, 0.95, epsilon=0.001)\n",
    "pi_small.run()\n",
    "best_policy = np.array(list(pi_small.policy))\n",
    "\n",
    "print(best_policy)\n",
    "\n",
    "# bestID = argmax(success_list)\n",
    "# best_policy = vi_data['policy'][bestRun]\n",
    "# best_policy = np.array(list(policy_list[bestID]))\n",
    "\n",
    "rows = env.nrow\n",
    "cols = env.ncol\n",
    "\n",
    "best_policy = best_policy.reshape(rows, cols)\n",
    "\n",
    "# plot the policy\n",
    "title='Frozen Lake PI Optimal Policy'\n",
    "plot_lake(env, best_policy, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.25\n",
    "gamma = 0.8  \n",
    "episodes = 100000\n",
    "epsilon = 1\n",
    "\n",
    "# random_map = generate_random_map(size=5, p=0.8)\n",
    "# env_large = gym.make('FrozenLake-v1', desc=random_map)\n",
    "# env = env_large.unwrapped\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "for episode in range(episodes):\n",
    "    if episode%5000 == 0:\n",
    "        print(episode)\n",
    "        print(Q)\n",
    "    state = env.reset()\n",
    "    complete = False\n",
    "    total_reward = 0\n",
    "    max_steps = 1000000\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        if complete:\n",
    "            break\n",
    "        current = state\n",
    "        if np.random.rand() < (epsilon):\n",
    "            action = np.argmax(Q[current, :])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        state, reward, complete, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        Q[current, action] += alpha * (reward + gamma * np.max(Q[state, :]) - Q[current, action])\n",
    "    epsilon = max([1 - 0.005*(episode/100), 0.1])\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_policy = np.reshape(np.argmax(Q, axis=1), -1)\n",
    "\n",
    "rows = env.nrow\n",
    "cols = env.ncol\n",
    "\n",
    "best_policy = best_policy.reshape(rows, cols)\n",
    "\n",
    "# plot the policy\n",
    "title='Frozen Lake Q Learning Optimal Policy'\n",
    "plot_lake(env, best_policy, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison - best policy: time, step, success\n",
    "start = time.time()\n",
    "steps, steps_stddev, successs = get_score(env, np.array(list(vi_small.policy)))\n",
    "end = time.time()\n",
    "print(steps, successs, end-start)\n",
    "\n",
    "start = time.time()\n",
    "steps, steps_stddev, successs = get_score(env, np.array(list(pi_small.policy)))\n",
    "end = time.time()\n",
    "print(steps, successs, end-start)\n",
    "\n",
    "start = time.time()\n",
    "steps, steps_stddev, successs = get_score(env, np.reshape(np.argmax(Q, axis=1), -1))\n",
    "end = time.time()\n",
    "print(steps, successs, end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the training time\n",
    "start = time.time()\n",
    "vi_small = ValueIteration(T, R, 0.95, epsilon=0.001)\n",
    "vi_small.run()\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "start = time.time()\n",
    "pi_small = PolicyIterationModified(T, R, 0.95, epsilon=0.001)\n",
    "pi_small.run()\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "alpha = 0.25\n",
    "gamma = 0.8  \n",
    "episodes = 100000\n",
    "epsilon = 1\n",
    "\n",
    "# random_map = generate_random_map(size=5, p=0.8)\n",
    "# env_large = gym.make('FrozenLake-v1', desc=random_map)\n",
    "# env = env_large.unwrapped\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    complete = False\n",
    "    total_reward = 0\n",
    "    max_steps = 1000000\n",
    "    \n",
    "    prev = Q\n",
    "    for i in range(max_steps):\n",
    "        if complete:\n",
    "            break\n",
    "        current = state\n",
    "        if np.random.rand() < (epsilon):\n",
    "            action = np.argmax(Q[current, :])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        state, reward, complete, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        Q[current, action] += alpha * (reward + gamma * np.max(Q[state, :]) - Q[current, action])\n",
    "    epsilon = max([1 - 0.005*(episode/100), 0.1])\n",
    "    if episode > 20000 and np.abs(np.mean(prev-Q)) < 0.001:\n",
    "        print(prev)\n",
    "        print(Q)\n",
    "        break\n",
    "    \n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the bigger size one\n",
    "random_map = generate_random_map(size=30, p=0.8)\n",
    "env2 = gym.make('FrozenLake-v1', desc=random_map).unwrapped\n",
    "\n",
    "env2.max_episode_steps=250\n",
    "\n",
    "# Create transition and reward matrices from OpenAI P matrix\n",
    "rows = env2.nrow\n",
    "cols = env2.ncol\n",
    "T = np.zeros((4, rows*cols, rows*cols))\n",
    "R = np.zeros((4, rows*cols, rows*cols))\n",
    "\n",
    "old_state = np.inf\n",
    "\n",
    "for square in env2.P:\n",
    "    for action in env2.P[square]:\n",
    "        for i in range(len(env2.P[square][action])):\n",
    "            new_state = env2.P[square][action][i][1]\n",
    "            if new_state == old_state:\n",
    "                T[action][square][env2.P[square][action][i][1]] = T[action][square][old_state] + env2.P[square][action][i][0]\n",
    "                R[action][square][env2.P[square][action][i][1]] = R[action][square][old_state] + env2.P[square][action][i][2]\n",
    "            else:\n",
    "                T[action][square][env2.P[square][action][i][1]] = env2.P[square][action][i][0]\n",
    "                R[action][square][env2.P[square][action][i][1]] = env2.P[square][action][i][2]\n",
    "            old_state = env2.P[square][action][i][1]\n",
    "\n",
    "print(env2.nrow, env2.ncol)\n",
    "plot_lake(env2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
