{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import hiive.mdptoolbox\n",
    "import hiive.mdptoolbox.mdp\n",
    "import hiive.mdptoolbox.example\n",
    "\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "import hiive_openAI_extract\n",
    "\n",
    "# import hiive.mdptoolbox as mdptoolbox\n",
    "from hiive.mdptoolbox.mdp import ValueIteration, PolicyIterationModified, QLearning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_problem(size):\n",
    "    random_map = generate_random_map(size=size, p=0.8)\n",
    "    env = gym.make('FrozenLake-v1', desc=random_map).unwrapped\n",
    "\n",
    "    env.max_episode_steps=250\n",
    "\n",
    "    # Create transition and reward matrices from OpenAI P matrix\n",
    "    rows = env.nrow\n",
    "    cols = env.ncol\n",
    "    T = np.zeros((4, rows*cols, rows*cols))\n",
    "    R = np.zeros((4, rows*cols, rows*cols))\n",
    "\n",
    "    old_state = np.inf\n",
    "\n",
    "    for square in env.P:\n",
    "        for action in env.P[square]:\n",
    "            for i in range(len(env.P[square][action])):\n",
    "                new_state = env.P[square][action][i][1]\n",
    "                if new_state == old_state:\n",
    "                    T[action][square][env.P[square][action][i][1]] = T[action][square][old_state] + env.P[square][action][i][0]\n",
    "                    R[action][square][env.P[square][action][i][1]] = R[action][square][old_state] + env.P[square][action][i][2]\n",
    "                else:\n",
    "                    T[action][square][env.P[square][action][i][1]] = env.P[square][action][i][0]\n",
    "                    R[action][square][env.P[square][action][i][1]] = env.P[square][action][i][2]\n",
    "                old_state = env.P[square][action][i][1]\n",
    "\n",
    "    print(env.nrow, env.ncol)\n",
    "#     plot_lake(env)\n",
    "    \n",
    "    return T, R, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration - change problem size\n",
    "time_list = []\n",
    "iter_list = []\n",
    "reward_list = []\n",
    "\n",
    "for size in range(5, 35, 5):\n",
    "    T, R, _ = initialize_problem(size)\n",
    "    \n",
    "    one_test = ValueIteration(T, R, gamma=0.98, epsilon=0.001, max_iter=100000)\n",
    "    \n",
    "    one_test.run()\n",
    "    time_list.append(one_test.time)\n",
    "    iter_list.append(one_test.iter)\n",
    "    reward_list.append(np.mean(one_test.V))\n",
    "\n",
    "print(time_list)\n",
    "print(iter_list)\n",
    "print(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration - large and small\n",
    "T_small, R_small, _ = initialize_problem(5)\n",
    "T_large, R_large, _ = initialize_problem(30)\n",
    "\n",
    "time_list1, time_list2 = [], []\n",
    "iter_list1, iter_list2 = [], []\n",
    "reward_list1, reward_list2 = [], []\n",
    "\n",
    "for gamma in range(100):\n",
    "    vi_small = ValueIteration(T_small, R_small, (gamma + 0.5) / 100, epsilon=0.001)\n",
    "    vi_small.run()\n",
    "    time_list1.append(vi_small.time)\n",
    "    iter_list1.append(vi_small.iter)\n",
    "    reward_list1.append(np.mean(vi_small.V))\n",
    "    \n",
    "    vi_large = ValueIteration(T_large, R_large, (gamma + 0.5) / 100, epsilon=0.00001)\n",
    "    vi_large.run()\n",
    "    time_list2.append(vi_large.time)\n",
    "    iter_list2.append(vi_large.iter)\n",
    "    reward_list2.append(np.mean(vi_large.V))\n",
    "\n",
    "gamma_arr = [(i + 0.5) / 100 for i in range(100)]   \n",
    "    \n",
    "plt.plot(gamma_arr, iter_list1, label='5*5 states')\n",
    "plt.plot(gamma_arr, iter_list2, label='30*30 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('Iterations to Converge')\n",
    "plt.title('Value Iteration - Iterations to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Frozen_Lake_vi_iters', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gamma_arr, time_list1, label='5*5 states')\n",
    "plt.plot(gamma_arr, time_list2, label='30*30 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('Clock time to Converge')\n",
    "plt.title('Value Iteration - Clock time to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Frozen_Lake_vi_time', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gamma_arr, reward_list1, label='5*5 states')\n",
    "plt.plot(gamma_arr, reward_list2, label='30*30 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('mean reward')\n",
    "plt.title('Value Iteration - mean rewards')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Frozen_Lake_vi_reward', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration - change problem size\n",
    "time_list = []\n",
    "iter_list = []\n",
    "reward_list = []\n",
    "\n",
    "for size in range(5, 35, 5):\n",
    "    T, R, _ = initialize_problem(size)\n",
    "    \n",
    "    one_test = PolicyIterationModified(T, R, gamma=0.98, epsilon=0.001, max_iter=100000)\n",
    "    \n",
    "    one_test.run()\n",
    "    time_list.append(one_test.time)\n",
    "    iter_list.append(one_test.iter)\n",
    "    reward_list.append(np.mean(one_test.V))\n",
    "\n",
    "print(time_list)\n",
    "print(iter_list)\n",
    "print(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration - large and small\n",
    "T_small, R_small, _ = initialize_problem(5)\n",
    "T_large, R_large, _ = initialize_problem(30)\n",
    "\n",
    "time_list1, time_list2 = [], []\n",
    "iter_list1, iter_list2 = [], []\n",
    "reward_list1, reward_list2 = [], []\n",
    "\n",
    "for gamma in range(100):\n",
    "    vi_small = PolicyIterationModified(T_small, R_small, (gamma + 0.5) / 100, epsilon=0.01)\n",
    "    vi_small.run()\n",
    "    time_list1.append(vi_small.time)\n",
    "    iter_list1.append(vi_small.iter)\n",
    "    reward_list1.append(np.mean(vi_small.V))\n",
    "    \n",
    "    vi_large = PolicyIterationModified(T_large, R_large, (gamma + 0.5) / 100, epsilon=0.01)\n",
    "    vi_large.run()\n",
    "    time_list2.append(vi_large.time)\n",
    "    iter_list2.append(vi_large.iter)\n",
    "    reward_list2.append(np.mean(vi_large.V))\n",
    "\n",
    "gamma_arr = [(i + 0.5) / 100 for i in range(100)]   \n",
    "    \n",
    "plt.plot(gamma_arr, iter_list1, label='5*5 states')\n",
    "plt.plot(gamma_arr, iter_list2, label='30*30 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('Iterations to Converge')\n",
    "plt.title('Policy Iteration - Iterations to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Frozen_Lake_pi_iters', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gamma_arr, time_list1, label='5*5 states')\n",
    "plt.plot(gamma_arr, time_list2, label='30*30 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('Clock time to Converge')\n",
    "plt.title('Policy Iteration - Clock time to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Frozen_Lake_pi_time', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gamma_arr, reward_list1, label='5*5 states')\n",
    "plt.plot(gamma_arr, reward_list2, label='30*30 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('mean reward')\n",
    "plt.title('Policy Iteration - mean rewards')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Frozen_Lake_pi_reward', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q learning - change problem size\n",
    "\n",
    "\n",
    "# time_list = []\n",
    "# iter_list = []\n",
    "# reward_list = []\n",
    "\n",
    "# for size in range(5, 35, 5):\n",
    "#     T, R, _ = initialize_problem(size)\n",
    "    \n",
    "#     one_test = QLearning(T, R, gamma=0.9, alpha=0.1, n_iter=100000)\n",
    "    \n",
    "#     runs = one_test.run()\n",
    "#     print(runs[-1]['Time'])\n",
    "#     time_list.append(runs[-1]['Time'])\n",
    "#     iter_list.append(runs[-1]['Iteration'])\n",
    "#     reward_list.append(runs[-1]['Mean V'])\n",
    "\n",
    "# print(time_list)\n",
    "# print(iter_list)\n",
    "# print(reward_list)\n",
    "\n",
    "reward_list = []\n",
    "iter_list = []\n",
    "time_array = []\n",
    "alpha = 0.1\n",
    "gamma = 0.98\n",
    "episodes = 100000\n",
    "# epsilon=1\n",
    "\n",
    "for size in range(5, 35, 5):\n",
    "    random_map = generate_random_map(size=size, p=0.8)\n",
    "    env_large = gym.make('FrozenLake-v1', desc=random_map)\n",
    "    env = env_large.unwrapped\n",
    "    \n",
    "    rewards = []\n",
    "    iters = []\n",
    "    \n",
    "    start = time.time()\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        if episode%5000 == 0:\n",
    "            print(episode)\n",
    "        state = env.reset()\n",
    "        complete = False\n",
    "        total_reward = 0\n",
    "        max_steps = 1000000\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            if complete:\n",
    "                break\n",
    "            current = state\n",
    "            if np.random.rand() < (0.5):\n",
    "                action = np.argmax(Q[current, :])\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            state, reward, complete, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            Q[current, action] += alpha * (reward + gamma * np.max(Q[state, :]) - Q[current, action])\n",
    "#         epsilon = max([1 - 0.005*(episode/100), 0.1])\n",
    "        rewards.append(total_reward)\n",
    "        iters.append(i)\n",
    "    reward_list.append(np.mean(rewards))\n",
    "    iter_list.append(np.mean(iters))\n",
    "    end = time.time()\n",
    "    time_array.append(end - start)\n",
    "    \n",
    "print(time_array)\n",
    "print(iter_list)\n",
    "print(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "iter_list = []\n",
    "time_array = []\n",
    "alphas = [i / 100 for i in range(5, 55, 5)]  \n",
    "gamma = 0.98\n",
    "episodes = 20000\n",
    "# epsilon=1\n",
    "\n",
    "for alpha in alphas:\n",
    "    random_map = generate_random_map(size=5, p=0.8)\n",
    "    env_large = gym.make('FrozenLake-v1', desc=random_map)\n",
    "    env = env_large.unwrapped\n",
    "    \n",
    "    rewards = []\n",
    "    iters = []\n",
    "    \n",
    "    start = time.time()\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        if episode%5000 == 0:\n",
    "            print(episode)\n",
    "        state = env.reset()\n",
    "        complete = False\n",
    "        total_reward = 0\n",
    "        max_steps = 1000000\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            if complete:\n",
    "                break\n",
    "            current = state\n",
    "            if np.random.rand() < (0.5):\n",
    "                action = np.argmax(Q[current, :])\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            state, reward, complete, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            Q[current, action] += alpha * (reward + gamma * np.max(Q[state, :]) - Q[current, action])\n",
    "#         epsilon = max([1 - 0.005*(episode/100), 0.1])\n",
    "        rewards.append(total_reward)\n",
    "        iters.append(i)\n",
    "    reward_list.append(np.mean(rewards))\n",
    "    iter_list.append(np.mean(iters))\n",
    "    end = time.time()\n",
    "    time_array.append(end - start)\n",
    "    \n",
    "print(time_array)\n",
    "print(iter_list)\n",
    "print(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [i / 100 for i in range(5, 55, 5)]  \n",
    "\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "\n",
    "plt.plot(alphas, iter_list,'-o',color='g', label='5*5 states')\n",
    "plt.xlabel('alpha (learning rate)')\n",
    "plt.ylabel('Iterations to Converge')\n",
    "x_major_locator=MultipleLocator(0.05)\n",
    "ax=plt.gca()\n",
    "ax.xaxis.set_major_locator(x_major_locator)\n",
    "plt.title('Q Learning - Iterations to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Frozen_Lake_q_alpha_iter.png', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(alphas, time_array,'-o',color='g',  label='5*5 states')\n",
    "plt.xlabel('alpha (learning rate)')\n",
    "plt.ylabel('Clock time to Converge')\n",
    "x_major_locator=MultipleLocator(0.05)\n",
    "ax=plt.gca()\n",
    "ax.xaxis.set_major_locator(x_major_locator)\n",
    "plt.title('Q Learning - Clock time to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Frozen_Lake_q_alpha_time.png', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(alphas, reward_list,'-o',color='g',  label='5*5 states')\n",
    "plt.xlabel('alpha (learning rate)')\n",
    "plt.ylabel('mean reward')\n",
    "x_major_locator=MultipleLocator(0.05)\n",
    "ax=plt.gca()\n",
    "ax.xaxis.set_major_locator(x_major_locator)\n",
    "plt.title('Q Learning - average rewards')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Frozen_Lake_q_alpha_reward.png', dpi=400)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "iter_list = []\n",
    "time_array = []\n",
    "alpha = 0.4\n",
    "gammas = [i / 100 for i in range(2, 100, 2)]  \n",
    "episodes = 20000\n",
    "# epsilon=1\n",
    "\n",
    "for gamma in gammas:\n",
    "    random_map = generate_random_map(size=5, p=0.8)\n",
    "    env_large = gym.make('FrozenLake-v1', desc=random_map)\n",
    "    env = env_large.unwrapped\n",
    "    \n",
    "    rewards = []\n",
    "    iters = []\n",
    "    \n",
    "    start = time.time()\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        if episode%5000 == 0:\n",
    "            print(episode)\n",
    "        state = env.reset()\n",
    "        complete = False\n",
    "        total_reward = 0\n",
    "        max_steps = 1000000\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            if complete:\n",
    "                break\n",
    "            current = state\n",
    "            if np.random.rand() < (0.5):\n",
    "                action = np.argmax(Q[current, :])\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            state, reward, complete, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            Q[current, action] += alpha * (reward + gamma * np.max(Q[state, :]) - Q[current, action])\n",
    "#         epsilon = max([1 - 0.005*(episode/100), 0.1])\n",
    "        rewards.append(total_reward)\n",
    "        iters.append(i)\n",
    "    reward_list.append(np.mean(rewards))\n",
    "    iter_list.append(np.mean(iters))\n",
    "    end = time.time()\n",
    "    time_array.append(end - start)\n",
    "    \n",
    "print(time_array)\n",
    "print(iter_list)\n",
    "print(reward_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gammas, iter_list,color='sandybrown', label='5*5 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('Iterations to Converge')\n",
    "plt.title('Q Learning - Clock time to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Frozen_Lake_q_gamma_iter.png', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gammas, time_array,color='sandybrown',  label='5*5 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('Clock time to Converge')\n",
    "plt.title('Q Learning - Clock time to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Frozen_Lake_q_gamma_time.png', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gammas, reward_list,color='sandybrown',  label='5*5 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('mean reward')\n",
    "plt.title('Q Learning - average rewards')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Frozen_Lake_q_gamma_reward.png', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "iter_list = []\n",
    "time_array = []\n",
    "alpha = 0.25\n",
    "gamma = 0.8  \n",
    "episodes = 20000\n",
    "epsilon = 1\n",
    "\n",
    "for strategy in range(4):\n",
    "    random_map = generate_random_map(size=5, p=0.8)\n",
    "    env_large = gym.make('FrozenLake-v1', desc=random_map)\n",
    "    env = env_large.unwrapped\n",
    "    \n",
    "    rewards = []\n",
    "    iters = []\n",
    "    \n",
    "    start = time.time()\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        if episode%5000 == 0:\n",
    "            print(episode)\n",
    "#             print(Q)\n",
    "        state = env.reset()\n",
    "        complete = False\n",
    "        total_reward = 0\n",
    "        max_steps = 1000000\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            if complete:\n",
    "                break\n",
    "            current = state\n",
    "            if np.random.rand() < (epsilon):\n",
    "                action = np.argmax(Q[current, :])\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            state, reward, complete, info = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            Q[current, action] += alpha * (reward + gamma * np.max(Q[state, :]) - Q[current, action])\n",
    "        # four different strategy\n",
    "        if strategy == 0:\n",
    "            epsilon = max([0.95**(episode/100), 0.1])\n",
    "        elif strategy == 1:\n",
    "            epsilon = max([1 - 0.005*(episode/100), 0.1])\n",
    "        elif strategy == 2:\n",
    "            epsilon = max([np.exp(-0.005*episode), 0.1])\n",
    "        else:\n",
    "            epsilon = 0.5\n",
    "        \n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        iters.append(i)\n",
    "    reward_list.append(np.mean(rewards))\n",
    "    iter_list.append(np.mean(iters))\n",
    "    end = time.time()\n",
    "    time_array.append(end - start)\n",
    "    \n",
    "print(time_array)\n",
    "print(iter_list)\n",
    "print(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules = [\"GeomDecay\", \"ArithDecay\", \"ExpDecay\", \"constant = 0.5\"]\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax  = sns.barplot(schedules,time_array) \n",
    "ax.set_title('Q Learning - Clock time to Converge')\n",
    "ax.set_xlabel('epsilon decay schedule')\n",
    "ax.set_ylabel('clock time')\n",
    "\n",
    "plt.savefig('Frozen_Lake_q_epsilon_time.png', dpi=400)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax  = sns.barplot(schedules,iter_list) \n",
    "ax.set_title('Q Learning - Iterations to Converge')\n",
    "ax.set_xlabel('epsilon decay schedule')\n",
    "ax.set_ylabel('iterations')\n",
    "\n",
    "plt.savefig('Frozen_Lake_q_epsilon_iter.png', dpi=400)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax  = sns.barplot(schedules,reward_list) \n",
    "ax.set_title('Q Learning - average rewards')\n",
    "ax.set_xlabel('epsilon decay schedule')\n",
    "ax.set_ylabel('reward')\n",
    "\n",
    "plt.savefig('Frozen_Lake_q_epsilon_reward.png', dpi=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
