{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import hiive.mdptoolbox\n",
    "import hiive.mdptoolbox.mdp\n",
    "import hiive.mdptoolbox.example\n",
    "\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "import hiive_openAI_extract\n",
    "\n",
    "# import hiive.mdptoolbox as mdptoolbox\n",
    "from hiive.mdptoolbox.mdp import ValueIteration, PolicyIterationModified, QLearning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# set seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration - change problem size\n",
    "time_list = []\n",
    "iter_list = []\n",
    "reward_list = []\n",
    "\n",
    "for size in [10, 50, 100, 200, 400, 500, 750, 1000]:\n",
    "    T, R = hiive.mdptoolbox.example.forest(S=size)\n",
    "    \n",
    "    one_test = ValueIteration(T, R, gamma=0.95, epsilon=0.001, max_iter=100000)\n",
    "    \n",
    "    one_test.run()\n",
    "    time_list.append(one_test.time)\n",
    "    iter_list.append(one_test.iter)\n",
    "    reward_list.append(np.mean(one_test.V))\n",
    "\n",
    "print(time_list)\n",
    "print(iter_list)\n",
    "print(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration - large and small\n",
    "T_small, R_small = hiive.mdptoolbox.example.forest(10)\n",
    "T_large, R_large = hiive.mdptoolbox.example.forest(1000)\n",
    "T_middle, R_middle = hiive.mdptoolbox.example.forest(625)\n",
    "\n",
    "time_list1, time_list2, time_list3 = [], [], []\n",
    "iter_list1, iter_list2, iter_list3 = [], [], []\n",
    "reward_list1, reward_list2, reward_list3 = [], [], []\n",
    "\n",
    "for gamma in range(100):\n",
    "    vi_small = ValueIteration(T_small, R_small, (gamma + 0.5) / 100, epsilon=0.01)\n",
    "    vi_small.run()\n",
    "    time_list1.append(vi_small.time)\n",
    "    iter_list1.append(vi_small.iter)\n",
    "    reward_list1.append(np.mean(vi_small.V))\n",
    "    \n",
    "    vi_large = ValueIteration(T_large, R_large, (gamma + 0.5) / 100, epsilon=0.01)\n",
    "    vi_large.run()\n",
    "    time_list2.append(vi_large.time)\n",
    "    iter_list2.append(vi_large.iter)\n",
    "    reward_list2.append(np.mean(vi_large.V))\n",
    "    \n",
    "    vi_middle = ValueIteration(T_middle, R_middle, (gamma + 0.5) / 100, epsilon=0.01)\n",
    "    vi_middle.run()\n",
    "    time_list3.append(vi_middle.time)\n",
    "    iter_list3.append(vi_middle.iter)\n",
    "    reward_list3.append(np.mean(vi_middle.V))\n",
    "\n",
    "gamma_arr = [(i + 0.5) / 100 for i in range(100)]   \n",
    "    \n",
    "plt.plot(gamma_arr, iter_list1, label='10 states')\n",
    "plt.plot(gamma_arr, iter_list2, label='1000 states')\n",
    "plt.plot(gamma_arr, iter_list3, label='625 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('Iterations to Converge')\n",
    "plt.title('Value Iteration - Iterations to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Forest_Manage_vi_iters', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gamma_arr, time_list1, label='10 states')\n",
    "plt.plot(gamma_arr, time_list2, label='1000 states')\n",
    "plt.plot(gamma_arr, time_list3, label='625 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('Clock time to Converge')\n",
    "plt.title('Value Iteration - Clock time to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Forest_Manage_vi_time', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gamma_arr, reward_list1, label='10 states')\n",
    "plt.plot(gamma_arr, reward_list2, label='1000 states')\n",
    "plt.plot(gamma_arr, reward_list3, label='625 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('mean reward')\n",
    "plt.title('Value Iteration - mean rewards')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Forest_Manage_vi_reward', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration - change problem size\n",
    "time_list = []\n",
    "iter_list = []\n",
    "reward_list = []\n",
    "\n",
    "for size in [10, 50, 100, 200, 400, 500, 750, 1000]:\n",
    "    T, R = hiive.mdptoolbox.example.forest(S=size)\n",
    "    \n",
    "    one_test = PolicyIterationModified(T, R, gamma=0.95, epsilon=0.001, max_iter=100000)\n",
    "    \n",
    "    one_test.run()\n",
    "    time_list.append(one_test.time)\n",
    "    iter_list.append(one_test.iter)\n",
    "    reward_list.append(np.mean(one_test.V))\n",
    "\n",
    "print(time_list)\n",
    "print(iter_list)\n",
    "print(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration - large and small\n",
    "T_small, R_small = hiive.mdptoolbox.example.forest(10)\n",
    "T_large, R_large = hiive.mdptoolbox.example.forest(1000)\n",
    "T_middle, R_middle = hiive.mdptoolbox.example.forest(625)\n",
    "\n",
    "time_list1, time_list2, time_list3 = [], [], []\n",
    "iter_list1, iter_list2, iter_list3 = [], [], []\n",
    "reward_list1, reward_list2, reward_list3 = [], [], []\n",
    "\n",
    "for gamma in range(100):\n",
    "    pi_small = PolicyIterationModified(T_small, R_small, (gamma + 0.5) / 100, epsilon=0.01)\n",
    "    pi_small.run()\n",
    "    time_list1.append(pi_small.time)\n",
    "    iter_list1.append(pi_small.iter)\n",
    "    reward_list1.append(np.mean(pi_small.V))\n",
    "    \n",
    "    pi_large = PolicyIterationModified(T_large, R_large, (gamma + 0.5) / 100, epsilon=0.01)\n",
    "    pi_large.run()\n",
    "    time_list2.append(pi_large.time)\n",
    "    iter_list2.append(pi_large.iter)\n",
    "    reward_list2.append(np.mean(pi_large.V))\n",
    "    \n",
    "    pi_middle = PolicyIterationModified(T_middle, R_middle, (gamma + 0.5) / 100, epsilon=0.01)\n",
    "    pi_middle.run()\n",
    "    time_list3.append(pi_middle.time)\n",
    "    iter_list3.append(pi_middle.iter)\n",
    "    reward_list3.append(np.mean(pi_middle.V))\n",
    "\n",
    "gamma_arr = [(i + 0.5) / 100 for i in range(100)]   \n",
    "    \n",
    "plt.plot(gamma_arr, iter_list1, label='10 states')\n",
    "plt.plot(gamma_arr, iter_list2, label='1000 states')\n",
    "plt.plot(gamma_arr, iter_list3, label='625 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('Iterations to Converge')\n",
    "plt.title('Policy Iteration - Iterations to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Forest_Manage_pi_iters', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gamma_arr, time_list1, label='10 states')\n",
    "plt.plot(gamma_arr, time_list2, label='1000 states')\n",
    "plt.plot(gamma_arr, time_list3, label='625 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('Clock time to Converge')\n",
    "plt.title('Policy Iteration - Clock time to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Forest_Manage_pi_time', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gamma_arr, reward_list1, label='10 states')\n",
    "plt.plot(gamma_arr, reward_list2, label='1000 states')\n",
    "plt.plot(gamma_arr, reward_list3, label='625 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('mean reward')\n",
    "plt.title('Policy Iteration - mean rewards')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Forest_Manage_pi_reward', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration - change problem size\n",
    "time_list = []\n",
    "iter_list = []\n",
    "reward_list = []\n",
    "\n",
    "for size in [10, 50, 100, 200, 400, 500, 750, 1000]:\n",
    "    T, R = hiive.mdptoolbox.example.forest(S=size)\n",
    "    \n",
    "    one_test = QLearning(T, R, gamma=0.9, alpha=0.1, n_iter=100000)\n",
    "    \n",
    "    test = one_test.run()\n",
    "    time_list.append(test[-1][\"Time\"])\n",
    "    iter_list.append(test[-1][\"Iteration\"])\n",
    "    reward_list.append(test[-1][\"Mean V\"])\n",
    "\n",
    "print(time_list)\n",
    "print(iter_list)\n",
    "print(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q learning - large and small\n",
    "T_small, R_small = hiive.mdptoolbox.example.forest(10)\n",
    "T_large, R_large = hiive.mdptoolbox.example.forest(1000)\n",
    "T_middle, R_middle = hiive.mdptoolbox.example.forest(625)\n",
    "\n",
    "time_list1, time_list2, time_list3 = [], [], []\n",
    "iter_list1, iter_list2, iter_list3 = [], [], []\n",
    "reward_list1, reward_list2, reward_list3 = [], [], []\n",
    "\n",
    "for gamma in range(100):\n",
    "    q_small = QLearning(T_small, R_small, (gamma + 0.5) / 100, n_iter=10000)\n",
    "    test1 = q_small.run()\n",
    "    time_list1.append(test1[-1][\"Time\"])\n",
    "    iter_list1.append(test1[-1][\"Iteration\"])\n",
    "    reward_list1.append(np.mean(test1[-1][\"Mean V\"]))\n",
    "    \n",
    "    q_large = QLearning(T_large, R_large, (gamma + 0.5) / 100, n_iter=10000)\n",
    "    test2 = q_large.run()\n",
    "    time_list2.append(test2[-1][\"Time\"])\n",
    "    iter_list2.append(test2[-1][\"Iteration\"])\n",
    "    reward_list2.append(np.mean(test2[-1][\"Mean V\"]))\n",
    "    \n",
    "    q_middle = QLearning(T_middle, R_middle, (gamma + 0.5) / 100, n_iter=10000)\n",
    "    test3 = q_middle.run()\n",
    "    time_list3.append(test3[-1][\"Time\"])\n",
    "    iter_list3.append(test3[-1][\"Iteration\"])\n",
    "    reward_list3.append(np.mean(test3[-1][\"Mean V\"]))\n",
    "\n",
    "gamma_arr = [(i + 0.5) / 100 for i in range(100)]   \n",
    "    \n",
    "plt.plot(gamma_arr, iter_list1, label='10 states')\n",
    "plt.plot(gamma_arr, iter_list2, label='1000 states')\n",
    "plt.plot(gamma_arr, iter_list3, label='625 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('Iterations to Converge')\n",
    "plt.title('Q Learning - Iterations to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Forest_Manage_q_gamma_iters', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gamma_arr, time_list1, label='10 states')\n",
    "plt.plot(gamma_arr, time_list2, label='1000 states')\n",
    "plt.plot(gamma_arr, time_list3, label='625 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('Clock time to Converge')\n",
    "plt.title('Q Learning - Clock time to Converge')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Forest_Manage_q_gamma_time', dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gamma_arr, reward_list1, label='10 states')\n",
    "plt.plot(gamma_arr, reward_list2, label='1000 states')\n",
    "plt.plot(gamma_arr, reward_list3, label='625 states')\n",
    "plt.xlabel('gamma (discount factor)')\n",
    "plt.ylabel('mean reward')\n",
    "plt.title('Q Learning - mean rewards')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('Forest_Manage_q_gamma_reward', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table -- alpha and alpha decay\n",
    "alphas = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "alpha_decays = [0.999, 0.99, 0.95, 0.9]\n",
    "\n",
    "for alpha in alphas:\n",
    "    for alpha_decay in alpha_decays:\n",
    "        q_small = QLearning(T_small, R_small, gamma=0.9, alpha=alpha, alpha_decay=alpha_decay, n_iter=10000)\n",
    "        test1 = q_small.run()\n",
    "        print(\"10 states: \", alpha, alpha_decay, test1[-1][\"Time\"], test1[-1][\"Mean V\"])\n",
    "        \n",
    "        q_large = QLearning(T_large, R_large, gamma=0.9, alpha=alpha, alpha_decay=alpha_decay, n_iter=10000)\n",
    "        test2 = q_large.run()\n",
    "        print(\"1000 states: \", alpha, alpha_decay, test2[-1][\"Time\"], test2[-1][\"Mean V\"])\n",
    "        \n",
    "        q_middle = QLearning(T_middle, R_middle, gamma=0.9, alpha=alpha, alpha_decay=alpha_decay, n_iter=10000)\n",
    "        test3 = q_middle.run()\n",
    "        print(\"625 states: \", alpha, alpha_decay, test3[-1][\"Time\"], test3[-1][\"Mean V\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot -- epsilon_decay\n",
    "epsilon_decays = [0.999, 0.99, 0.95, 0.9]\n",
    "\n",
    "T_large, R_large = hiive.mdptoolbox.example.forest(1000)\n",
    "T_middle, R_middle = hiive.mdptoolbox.example.forest(625)\n",
    "\n",
    "time_list = []\n",
    "reward_list = []\n",
    "\n",
    "for epsilon_decay in epsilon_decays:\n",
    "#     q_small = QLearning(T_small, R_small, gamma=0.9, epsilon_decay=epsilon_decay, n_iter=100000)\n",
    "#     test1 = q_small.run()\n",
    "#     time_list.append(test1[-1][\"Time\"])\n",
    "#     reward_list.append(test1[-1][\"Mean V\"])\n",
    "    \n",
    "    q_large = QLearning(T_large, R_large, gamma=0.9, epsilon_decay=epsilon_decay, n_iter=100000)\n",
    "    test2 = q_large.run()\n",
    "    time_list.append(test2[-1][\"Time\"])\n",
    "    reward_list.append(test2[-1][\"Mean V\"])\n",
    "\n",
    "    q_middle = QLearning(T_middle, R_middle, gamma=0.9, epsilon_decay=epsilon_decay, n_iter=100000)\n",
    "    test3 = q_middle.run()\n",
    "    time_list.append(test3[-1][\"Time\"])\n",
    "    reward_list.append(test3[-1][\"Mean V\"])\n",
    "\n",
    "# schedules = [\"0.999/10\", \"0.999/1000\", \"0.999/625\", \"0.99/10\", \"0.99/1000\", \"0.99/625\", \"0.95/10\", \"0.95/1000\",\"0.95/625\", \"0.9/10\",  \"0.9/1000\", \"0.9/625\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules = [\"0.999/1000\", \"0.999/625\", \"0.99/1000\", \"0.99/625\", \"0.95/1000\",\"0.95/625\",  \"0.9/1000\", \"0.9/625\"]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax  = sns.barplot(schedules,time_list) \n",
    "ax.set_title('Q Learning - Clock time to Converge')\n",
    "ax.set_xlabel('epsilon decay/# of states')\n",
    "ax.set_ylabel('clock time')\n",
    "ax.set_xticklabels(labels=schedules, rotation=20)\n",
    "\n",
    "plt.savefig('Forest_Manage_q_epsilon_time.png', dpi=400)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax  = sns.barplot(schedules,reward_list) \n",
    "ax.set_title('Q Learning - average rewards')\n",
    "ax.set_xlabel('epsilon decay/# of states')\n",
    "ax.set_ylabel('reward')\n",
    "ax.set_xticklabels(labels=schedules, rotation=20)\n",
    "\n",
    "plt.savefig('Forest_Manage_q_epsilon_reward.png', dpi=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
